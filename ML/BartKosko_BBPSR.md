# [Noise-Boosted Bidirectional Backpropagation and Adversarial Learning](https://www.sciencedirect.com/science/article/pii/S0893608019302771)

## [Using Noise to Speed Up Video Classification with Recurrent Backpropagation](https://ieeexplore.ieee.org/document/7965843)
## [Bidirectional Backpropagation](https://ieeexplore.ieee.org/document/8751137)

### Key Points

- The Bi-directional Associative Memory (BAM) theorem states that every real rectangular matrix M is bidirectionally stable for threshold or sigmoidal neurons: Any input stimulation quickly leads to an equilibrium or resonating bidirectional fixed point of a fixed input vector and a fixed output vector. 
- Noise Injection The BP algorithm trains a neural network to approximate some mapping from the input space X to the output space Y
- The BP algorithm is itself a special case of generalized expectationmaximization (GEM) for maximium-likelihood estimation with latent or hiddent parameters.
- These EM results show that injecting noise helps the maximum-likelihood system bound faster up the nearest hill of probability on average if the noise satisfies a positivity condition that involves a likelihood ratio.
- The Noisy Expectation-Maximization (NEM) theorem states the general sufficient condition for a noise benefit in the EM algorithm. The theorem shows that noise injection can only shorten the EM algorithmâ€™s walk up the nearest hill of loglikelihood on average if the noise satisfies the NEM positivity condition. It holds for additive or multiplicative or any other type of measurable noise injection.
